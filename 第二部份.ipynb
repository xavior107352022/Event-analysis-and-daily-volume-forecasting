{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取事件资料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_excel('MS_event17.xlsx')\n",
    "df_event['M_group'] = [ str(x)[4:6] for x in df_event.Date ]\n",
    "df_event['M_group_desc'] = [ str(x)[4:6] for x in df_event.Date_desclosure ]\n",
    "Event_list = df_event['Event.1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MS_PandQ=pd.read_csv('MS_PandQ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#匯入產業類別資料\n",
    "df_IND = pd.read_excel('MS_IND.xlsx')\n",
    "df_IND.columns = ['Code','Name','CSRC_IND_Code','Wind_IND_Code']\n",
    "len(df_IND['Wind_IND_Code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IND_list = df_IND['Wind_IND_Code'].unique()\n",
    "#      ['Wind制药、生物科技与生命科学', 'Wind材料Ⅱ', 'Wind耐用消费品与服装', 'Wind半导体与半导体生产设备',\n",
    "#       'Wind资本货物', 'Wind技术硬件与设备', 'Wind房地产Ⅱ', 'Wind能源Ⅱ', 'Wind零售业',\n",
    "#       'Wind媒体Ⅱ', 'Wind消费者服务Ⅱ', 'Wind商业和专业服务', 'Wind公用事业Ⅱ', 'Wind运输',\n",
    "#       'Wind食品、饮料与烟草', 'Wind医疗保健设备与服务', 'Wind汽车与汽车零部件', 'Wind软件与服务',\n",
    "#       'Wind家庭与个人用品', 'Wind银行', 'Wind食品与主要用品零售Ⅱ', 'Wind多元金融', 'Wind电信服务Ⅱ']\n",
    "for i in range(0,len(IND_list)):\n",
    "    df_IND['IND_dummy_'+str(i)] = ( df_IND['Wind_IND_Code'] == IND_list[i])*1 #dummy 1 是制药、生物科技与生命科学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MS_PQ_Ind = pd.merge(left=df_MS_PandQ,right=df_IND,on=['Code'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_MS_PQ_Ind) == len(df_MS_PandQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 創造市值dummy 共5類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Cap =pd.read_csv('Cap_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_MS_PQ_Ind_Cap = pd.merge(left = df_MS_PQ_Ind,right=df_Cap[['Date','Code','Tournover','Cap']],on=['Date','Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_MS_PQ_Ind_Cap) == len(df_MS_PQ_Ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_Quintile = df_MS_PQ_Ind_Cap.groupby('Date')['Cap'].apply(lambda x : np.where(np.array(x) < np.percentile(x,20),1,0))\n",
    "second_Quintile = df_MS_PQ_Ind_Cap.groupby('Date')['Cap'].apply( lambda x :np.where((np.array(x) >= np.percentile(x,20))&\n",
    "                                                                               (np.array(x) <  np.percentile(x,40))\n",
    "                                                                              ,1,0) )\n",
    "third_Quintile = df_MS_PQ_Ind_Cap.groupby('Date')['Cap'].apply( lambda x :np.where((np.array(x) >= np.percentile(x,40))&\n",
    "                                                                              (np.array(x) <  np.percentile(x,60))\n",
    "                                                                              ,1,0) )\n",
    "fourth_Quintile = df_MS_PQ_Ind_Cap.groupby('Date')['Cap'].apply( lambda x :np.where((np.array(x) >= np.percentile(x,60))&\n",
    "                                                                               (np.array(x) <  np.percentile(x,80))\n",
    "                                                                              ,1,0) )\n",
    "fifth_Quintile = df_MS_PQ_Ind_Cap.groupby('Date')['Cap'].apply(lambda x : np.where(np.array(x) >= np.percentile(x,80),1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由於上方是GroupbyDate 產出結果依照時間順序往下牌\n",
    "df_sortdate = df_MS_PQ_Ind_Cap.sort_values(['Date','Code'],ascending=[1,1]).reset_index(drop=True)\n",
    "df_sortdate['First_Quintile'] = [ y for x in first_Quintile for y in x]\n",
    "df_sortdate['Second_Quintile'] = [ y for x in second_Quintile for y in x]\n",
    "df_sortdate['Third_Quintile'] = [ y for x in third_Quintile for y in x]\n",
    "df_sortdate['Fourth_Quintile'] = [ y for x in fourth_Quintile for y in x]\n",
    "df_sortdate['Fifth_Quintile'] = [ y for x in fifth_Quintile for y in x]\n",
    "df_sortdate['Quintile_Class'] = df_sortdate['First_Quintile'].values*1+df_sortdate['Second_Quintile'].values*2+df_sortdate['Third_Quintile'].values*3+df_sortdate['Fourth_Quintile'].values*4+df_sortdate['Fifth_Quintile'].values*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "20130902    668\n",
      "20130903    668\n",
      "20130904    670\n",
      "20130905    671\n",
      "20130906    667\n",
      "20130909    666\n",
      "20130910    665\n",
      "20130911    666\n",
      "20130912    667\n",
      "20130913    667\n",
      "20130916    667\n",
      "20130917    667\n",
      "20130918    667\n",
      "20130923    666\n",
      "20130924    669\n",
      "20130925    667\n",
      "20130926    665\n",
      "20130927    667\n",
      "20130930    668\n",
      "20131008    666\n",
      "20131009    663\n",
      "20131010    666\n",
      "20131011    667\n",
      "20131014    664\n",
      "20131015    665\n",
      "20131016    662\n",
      "20131017    662\n",
      "20131018    663\n",
      "20131021    662\n",
      "20131022    663\n",
      "           ... \n",
      "20180613    843\n",
      "20180614    841\n",
      "20180615    840\n",
      "20180619    833\n",
      "20180620    833\n",
      "20180621    833\n",
      "20180622    834\n",
      "20180625    837\n",
      "20180626    839\n",
      "20180627    841\n",
      "20180628    843\n",
      "20180629    844\n",
      "20180702    847\n",
      "20180703    852\n",
      "20180704    852\n",
      "20180705    852\n",
      "20180706    855\n",
      "20180709    857\n",
      "20180710    858\n",
      "20180711    858\n",
      "20180712    858\n",
      "20180713    859\n",
      "20180716    859\n",
      "20180717    859\n",
      "20180718    859\n",
      "20180719    859\n",
      "20180720    861\n",
      "20180723    861\n",
      "20180724    861\n",
      "20180727    911\n",
      "Name: Code, Length: 1194, dtype: int64\n",
      "Date\n",
      "20130902    134\n",
      "20130903    134\n",
      "20130904    134\n",
      "20130905    134\n",
      "20130906    134\n",
      "20130909    133\n",
      "20130910    133\n",
      "20130911    133\n",
      "20130912    134\n",
      "20130913    134\n",
      "20130916    134\n",
      "20130917    134\n",
      "20130918    134\n",
      "20130923    133\n",
      "20130924    134\n",
      "20130925    134\n",
      "20130926    133\n",
      "20130927    134\n",
      "20130930    134\n",
      "20131008    133\n",
      "20131009    133\n",
      "20131010    133\n",
      "20131011    134\n",
      "20131014    133\n",
      "20131015    133\n",
      "20131016    133\n",
      "20131017    133\n",
      "20131018    133\n",
      "20131021    133\n",
      "20131022    133\n",
      "           ... \n",
      "20180613    169\n",
      "20180614    168\n",
      "20180615    168\n",
      "20180619    167\n",
      "20180620    167\n",
      "20180621    167\n",
      "20180622    167\n",
      "20180625    168\n",
      "20180626    168\n",
      "20180627    168\n",
      "20180628    169\n",
      "20180629    169\n",
      "20180702    170\n",
      "20180703    171\n",
      "20180704    171\n",
      "20180705    171\n",
      "20180706    171\n",
      "20180709    172\n",
      "20180710    172\n",
      "20180711    172\n",
      "20180712    172\n",
      "20180713    172\n",
      "20180716    172\n",
      "20180717    172\n",
      "20180718    172\n",
      "20180719    172\n",
      "20180720    172\n",
      "20180723    172\n",
      "20180724    172\n",
      "20180727    182\n",
      "Name: First_Quintile, Length: 1194, dtype: int64\n",
      "Date\n",
      "20130902    133\n",
      "20130903    133\n",
      "20130904    134\n",
      "20130905    134\n",
      "20130906    133\n",
      "20130909    133\n",
      "20130910    133\n",
      "20130911    133\n",
      "20130912    133\n",
      "20130913    133\n",
      "20130916    133\n",
      "20130917    133\n",
      "20130918    133\n",
      "20130923    133\n",
      "20130924    134\n",
      "20130925    133\n",
      "20130926    133\n",
      "20130927    133\n",
      "20130930    133\n",
      "20131008    133\n",
      "20131009    132\n",
      "20131010    133\n",
      "20131011    133\n",
      "20131014    133\n",
      "20131015    133\n",
      "20131016    132\n",
      "20131017    132\n",
      "20131018    132\n",
      "20131021    132\n",
      "20131022    132\n",
      "           ... \n",
      "20180613    168\n",
      "20180614    168\n",
      "20180615    168\n",
      "20180619    166\n",
      "20180620    166\n",
      "20180621    166\n",
      "20180622    167\n",
      "20180625    167\n",
      "20180626    168\n",
      "20180627    168\n",
      "20180628    168\n",
      "20180629    169\n",
      "20180702    169\n",
      "20180703    170\n",
      "20180704    170\n",
      "20180705    170\n",
      "20180706    171\n",
      "20180709    171\n",
      "20180710    171\n",
      "20180711    171\n",
      "20180712    171\n",
      "20180713    172\n",
      "20180716    172\n",
      "20180717    172\n",
      "20180718    172\n",
      "20180719    172\n",
      "20180720    172\n",
      "20180723    172\n",
      "20180724    172\n",
      "20180727    182\n",
      "Name: Second_Quintile, Length: 1194, dtype: int64\n",
      "Date\n",
      "20130902    134\n",
      "20130903    134\n",
      "20130904    134\n",
      "20130905    134\n",
      "20130906    133\n",
      "20130909    133\n",
      "20130910    133\n",
      "20130911    133\n",
      "20130912    133\n",
      "20130913    133\n",
      "20130916    133\n",
      "20130917    133\n",
      "20130918    133\n",
      "20130923    133\n",
      "20130924    132\n",
      "20130925    133\n",
      "20130926    133\n",
      "20130927    133\n",
      "20130930    134\n",
      "20131008    133\n",
      "20131009    133\n",
      "20131010    133\n",
      "20131011    133\n",
      "20131014    132\n",
      "20131015    133\n",
      "20131016    132\n",
      "20131017    132\n",
      "20131018    133\n",
      "20131021    132\n",
      "20131022    133\n",
      "           ... \n",
      "20180613    169\n",
      "20180614    168\n",
      "20180615    168\n",
      "20180619    167\n",
      "20180620    167\n",
      "20180621    167\n",
      "20180622    166\n",
      "20180625    167\n",
      "20180626    167\n",
      "20180627    168\n",
      "20180628    169\n",
      "20180629    168\n",
      "20180702    169\n",
      "20180703    170\n",
      "20180704    170\n",
      "20180705    170\n",
      "20180706    171\n",
      "20180709    171\n",
      "20180710    172\n",
      "20180711    172\n",
      "20180712    172\n",
      "20180713    171\n",
      "20180716    171\n",
      "20180717    171\n",
      "20180718    171\n",
      "20180719    171\n",
      "20180720    172\n",
      "20180723    172\n",
      "20180724    172\n",
      "20180727    182\n",
      "Name: Third_Quintile, Length: 1194, dtype: int64\n",
      "Date\n",
      "20130902    133\n",
      "20130903    133\n",
      "20130904    134\n",
      "20130905    134\n",
      "20130906    133\n",
      "20130909    133\n",
      "20130910    133\n",
      "20130911    133\n",
      "20130912    133\n",
      "20130913    133\n",
      "20130916    133\n",
      "20130917    133\n",
      "20130918    133\n",
      "20130923    133\n",
      "20130924    135\n",
      "20130925    133\n",
      "20130926    133\n",
      "20130927    133\n",
      "20130930    133\n",
      "20131008    133\n",
      "20131009    132\n",
      "20131010    133\n",
      "20131011    133\n",
      "20131014    133\n",
      "20131015    133\n",
      "20131016    132\n",
      "20131017    132\n",
      "20131018    132\n",
      "20131021    132\n",
      "20131022    132\n",
      "           ... \n",
      "20180613    168\n",
      "20180614    168\n",
      "20180615    168\n",
      "20180619    166\n",
      "20180620    166\n",
      "20180621    166\n",
      "20180622    167\n",
      "20180625    167\n",
      "20180626    168\n",
      "20180627    168\n",
      "20180628    168\n",
      "20180629    169\n",
      "20180702    169\n",
      "20180703    170\n",
      "20180704    170\n",
      "20180705    170\n",
      "20180706    171\n",
      "20180709    171\n",
      "20180710    171\n",
      "20180711    171\n",
      "20180712    171\n",
      "20180713    172\n",
      "20180716    172\n",
      "20180717    172\n",
      "20180718    172\n",
      "20180719    172\n",
      "20180720    172\n",
      "20180723    172\n",
      "20180724    172\n",
      "20180727    182\n",
      "Name: Fourth_Quintile, Length: 1194, dtype: int64\n",
      "Date\n",
      "20130902    134\n",
      "20130903    134\n",
      "20130904    134\n",
      "20130905    135\n",
      "20130906    134\n",
      "20130909    134\n",
      "20130910    133\n",
      "20130911    134\n",
      "20130912    134\n",
      "20130913    134\n",
      "20130916    134\n",
      "20130917    134\n",
      "20130918    134\n",
      "20130923    134\n",
      "20130924    134\n",
      "20130925    134\n",
      "20130926    133\n",
      "20130927    134\n",
      "20130930    134\n",
      "20131008    134\n",
      "20131009    133\n",
      "20131010    134\n",
      "20131011    134\n",
      "20131014    133\n",
      "20131015    133\n",
      "20131016    133\n",
      "20131017    133\n",
      "20131018    133\n",
      "20131021    133\n",
      "20131022    133\n",
      "           ... \n",
      "20180613    169\n",
      "20180614    169\n",
      "20180615    168\n",
      "20180619    167\n",
      "20180620    167\n",
      "20180621    167\n",
      "20180622    167\n",
      "20180625    168\n",
      "20180626    168\n",
      "20180627    169\n",
      "20180628    169\n",
      "20180629    169\n",
      "20180702    170\n",
      "20180703    171\n",
      "20180704    171\n",
      "20180705    171\n",
      "20180706    171\n",
      "20180709    172\n",
      "20180710    172\n",
      "20180711    172\n",
      "20180712    172\n",
      "20180713    172\n",
      "20180716    172\n",
      "20180717    172\n",
      "20180718    172\n",
      "20180719    172\n",
      "20180720    173\n",
      "20180723    173\n",
      "20180724    173\n",
      "20180727    183\n",
      "Name: Fifth_Quintile, Length: 1194, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_sortdate.groupby('Date')['Code'].count())\n",
    "print(df_sortdate.groupby('Date')['First_Quintile'].sum())\n",
    "print(df_sortdate.groupby('Date')['Second_Quintile'].sum())\n",
    "print(df_sortdate.groupby('Date')['Third_Quintile'].sum())\n",
    "print(df_sortdate.groupby('Date')['Fourth_Quintile'].sum())\n",
    "print(df_sortdate.groupby('Date')['Fifth_Quintile'].sum())#確定沒算錯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MS_PQ_Ind_Cap = df_sortdate.sort_values(['Code','Date'],ascending=[1,1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取前后五天分析\n",
    "for i in range(-30,11):\n",
    "    df_MS_PQ_Ind_Cap['Volume_t'+str(i)] = df_MS_PQ_Ind_Cap.groupby('Code')['Volume'].shift(-i)\n",
    "#取T+1开盘价分析\n",
    "df_MS_PQ_Ind_Cap['Open_t+1'] = df_MS_PQ_Ind_Cap.groupby('Code')['Open'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in greater\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "df_MS_PQ_Ind_Cap['MeanPast30V'] = np.mean(df_MS_PQ_Ind_Cap[['Volume_t-30',\n",
    "       'Volume_t-29', 'Volume_t-28', 'Volume_t-27', 'Volume_t-26',\n",
    "       'Volume_t-25', 'Volume_t-24', 'Volume_t-23', 'Volume_t-22',\n",
    "       'Volume_t-21', 'Volume_t-20', 'Volume_t-19', 'Volume_t-18',\n",
    "       'Volume_t-17', 'Volume_t-16', 'Volume_t-15', 'Volume_t-14',\n",
    "       'Volume_t-13', 'Volume_t-12', 'Volume_t-11', 'Volume_t-10',\n",
    "       'Volume_t-9', 'Volume_t-8', 'Volume_t-7', 'Volume_t-6', 'Volume_t-5',\n",
    "       'Volume_t-4', 'Volume_t-3', 'Volume_t-2', 'Volume_t-1']],axis=1)\n",
    "df_MS_PQ_Ind_Cap['StdPast30V'] = np.std(df_MS_PQ_Ind_Cap[['Volume_t-30',\n",
    "       'Volume_t-29', 'Volume_t-28', 'Volume_t-27', 'Volume_t-26',\n",
    "       'Volume_t-25', 'Volume_t-24', 'Volume_t-23', 'Volume_t-22',\n",
    "       'Volume_t-21', 'Volume_t-20', 'Volume_t-19', 'Volume_t-18',\n",
    "       'Volume_t-17', 'Volume_t-16', 'Volume_t-15', 'Volume_t-14',\n",
    "       'Volume_t-13', 'Volume_t-12', 'Volume_t-11', 'Volume_t-10', \n",
    "       'Volume_t-9', 'Volume_t-8', 'Volume_t-7', 'Volume_t-6', 'Volume_t-5',\n",
    "       'Volume_t-4', 'Volume_t-3', 'Volume_t-2', 'Volume_t-1']],axis=1)\n",
    "df_MS_PQ_Ind_Cap['UpperBound'] = df_MS_PQ_Ind_Cap['MeanPast30V'].values + df_MS_PQ_Ind_Cap['StdPast30V'].values\n",
    "df_MS_PQ_Ind_Cap['LowerBound'] = df_MS_PQ_Ind_Cap['MeanPast30V'].values - df_MS_PQ_Ind_Cap['StdPast30V'].values\n",
    "df_MS_PQ_Ind_Cap['HighThan1Std'] = np.where((df_MS_PQ_Ind_Cap['Volume'].values-df_MS_PQ_Ind_Cap['UpperBound'].values)>0,1,0)\n",
    "df_MS_PQ_Ind_Cap['LowThan1Std'] = np.where((df_MS_PQ_Ind_Cap['Volume'].values-df_MS_PQ_Ind_Cap['LowerBound'].values)<0,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#筛除缺少资料其间 与对其事件与价格量资料 5日\n",
    "df_MS_PQ_Ind_Cap=df_MS_PQ_Ind_Cap[(df_MS_PQ_Ind_Cap.Date.astype('int') > 20140720)&(df_MS_PQ_Ind_Cap.Date.astype('int') < 20180717)].reset_index(drop=True)\n",
    "df_event=df_event[df_event.Date.astype('int')<20180710].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705088\n",
      "108238\n"
     ]
    }
   ],
   "source": [
    "print(len(df_MS_PQ_Ind_Cap))\n",
    "print(len(df_event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將事件資料併入股價量價資料 left 量價 right 事件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108026\n",
      "108238\n"
     ]
    }
   ],
   "source": [
    "print(len(df_event.drop_duplicates().reset_index(drop=True)))\n",
    "print(len(df_event))#這兩個數有差異 表示有重複的事件資料\n",
    "df_event_dropdu = df_event.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_dropdu.Date = df_event_dropdu.Date.astype('str')\n",
    "df_MS_PQ_Ind_Cap.Date = df_MS_PQ_Ind_Cap.Date.astype('str')\n",
    "df_event_dropdu.Code = df_event_dropdu.Code.astype('str')\n",
    "df_MS_PQ_Ind_Cap.Code = df_MS_PQ_Ind_Cap.Code.astype('str')\n",
    "\n",
    "df_PandQ_Event = pd.merge(left=df_MS_PQ_Ind_Cap,right=df_event_dropdu ,on=['Date','Code'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Code</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Name_x</th>\n",
       "      <th>...</th>\n",
       "      <th>UpperBound</th>\n",
       "      <th>LowerBound</th>\n",
       "      <th>HighThan1Std</th>\n",
       "      <th>LowThan1Std</th>\n",
       "      <th>Name_y</th>\n",
       "      <th>Event</th>\n",
       "      <th>Event.1</th>\n",
       "      <th>Date_desclosure</th>\n",
       "      <th>M_group</th>\n",
       "      <th>M_group_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264432</th>\n",
       "      <td>326977</td>\n",
       "      <td>20140729</td>\n",
       "      <td>002306.SZ</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.13</td>\n",
       "      <td>23610421.0</td>\n",
       "      <td>144662896.0</td>\n",
       "      <td>*ST云网</td>\n",
       "      <td>...</td>\n",
       "      <td>3.176678e+07</td>\n",
       "      <td>876194.118012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>*ST云网</td>\n",
       "      <td>交易</td>\n",
       "      <td>复牌</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264433</th>\n",
       "      <td>326977</td>\n",
       "      <td>20140729</td>\n",
       "      <td>002306.SZ</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.13</td>\n",
       "      <td>23610421.0</td>\n",
       "      <td>144662896.0</td>\n",
       "      <td>*ST云网</td>\n",
       "      <td>...</td>\n",
       "      <td>3.176678e+07</td>\n",
       "      <td>876194.118012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>*ST云网</td>\n",
       "      <td>交易</td>\n",
       "      <td>大宗交易</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264434</th>\n",
       "      <td>326977</td>\n",
       "      <td>20140729</td>\n",
       "      <td>002306.SZ</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.13</td>\n",
       "      <td>23610421.0</td>\n",
       "      <td>144662896.0</td>\n",
       "      <td>*ST云网</td>\n",
       "      <td>...</td>\n",
       "      <td>3.176678e+07</td>\n",
       "      <td>876194.118012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>*ST云网</td>\n",
       "      <td>股權質押</td>\n",
       "      <td>股權質押公告</td>\n",
       "      <td>20140729.0</td>\n",
       "      <td>07</td>\n",
       "      <td>07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      Date       Code  Open  High   Low  Close      Volume  \\\n",
       "264432      326977  20140729  002306.SZ  6.13  6.13  6.03   6.13  23610421.0   \n",
       "264433      326977  20140729  002306.SZ  6.13  6.13  6.03   6.13  23610421.0   \n",
       "264434      326977  20140729  002306.SZ  6.13  6.13  6.03   6.13  23610421.0   \n",
       "\n",
       "             Amount Name_x      ...         UpperBound     LowerBound  \\\n",
       "264432  144662896.0  *ST云网      ...       3.176678e+07  876194.118012   \n",
       "264433  144662896.0  *ST云网      ...       3.176678e+07  876194.118012   \n",
       "264434  144662896.0  *ST云网      ...       3.176678e+07  876194.118012   \n",
       "\n",
       "        HighThan1Std  LowThan1Std  Name_y  Event  Event.1  Date_desclosure  \\\n",
       "264432             0            0   *ST云网     交易       复牌              NaN   \n",
       "264433             0            0   *ST云网     交易     大宗交易              NaN   \n",
       "264434             0            0   *ST云网   股權質押   股權質押公告       20140729.0   \n",
       "\n",
       "        M_group  M_group_desc  \n",
       "264432       07                \n",
       "264433       07                \n",
       "264434       07            07  \n",
       "\n",
       "[3 rows x 97 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PandQ_Event[((df_PandQ_Event.Date == '20140729') & (df_PandQ_Event.Code == '002306.SZ') )] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "49\n",
      "50\n",
      "50\n",
      "52\n",
      "53\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#测试有无为0得序列\n",
    "df_testzero = df_PandQ_Event[['Volume_t-5', 'Volume_t-4', 'Volume_t-3', 'Volume_t-2',\n",
    "       'Volume_t-1', 'Volume_t0', 'Volume_t1', 'Volume_t2', 'Volume_t3',\n",
    "       'Volume_t4', 'Volume_t5', 'Open_t+1']]\n",
    "for i in range(0,len(df_testzero.columns)):\n",
    "    print(sum((df_testzero.iloc[:,i] == 0)*1)) #很好都没有0的 下面就不会有无限大的可能安心算變動率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PandQ_Event['Chg_t~t+1%'] =( df_PandQ_Event['Volume'].values - df_PandQ_Event['Volume_t-1'].values)/df_PandQ_Event['Volume_t-1'].values\n",
    "df_PandQ_Event['E_dummy1'] = (df_PandQ_Event['Event.1'] == '大宗交易')*1\n",
    "df_PandQ_Event['E_dummy2'] = (df_PandQ_Event['Event.1'] == '股權質押公告')*1\n",
    "df_PandQ_Event['E_dummy3'] = (df_PandQ_Event['Event.1'] == '撤消ST')*1\n",
    "df_PandQ_Event['E_dummy4'] = (df_PandQ_Event['Event.1'] == '纳入重要指数')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "705088"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_MS_PQ_Ind_Cap))\n",
    "df_PandQ_Event_dropMultiEvent = df_PandQ_Event[['Code','Date', 'Close','Volume','HighThan1Std','LowThan1Std',\n",
    "                                                'Chg_t~t+1%','Open_t+1','High','Low','Wind_IND_Code','Volume_t-5', 'Volume_t-4', 'Volume_t-3', 'Volume_t-2',\n",
    "                                                'Volume_t-1', 'Volume_t0', 'Volume_t1', 'Volume_t2', 'Volume_t3','Volume_t4', 'Volume_t5',\n",
    "                                               'IND_dummy_0' ,'IND_dummy_1' , 'IND_dummy_2' , 'IND_dummy_3' ,'IND_dummy_4',\n",
    "                                               'IND_dummy_5' ,'IND_dummy_6' , 'IND_dummy_7' , 'IND_dummy_8' ,'IND_dummy_9', \n",
    "                                               'IND_dummy_10','IND_dummy_11', 'IND_dummy_12', 'IND_dummy_13','IND_dummy_14',\n",
    "                                               'IND_dummy_15', 'IND_dummy_16','IND_dummy_17', 'IND_dummy_18', 'IND_dummy_19', \n",
    "                                                'IND_dummy_20','IND_dummy_21', 'IND_dummy_22', \n",
    "                                                'Cap', 'First_Quintile','Second_Quintile', 'Third_Quintile', 'Fourth_Quintile','Fifth_Quintile','Quintile_Class'\n",
    "                                               ]].drop_duplicates().reset_index(drop=True) \n",
    "#長度必須等於8705088\n",
    "len(df_PandQ_Event_dropMultiEvent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_PandQ_Event_dropMultiEvent['E_dummy1'] = df_PandQ_Event.groupby(['Code','Date'])['E_dummy1'].sum().tolist()\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy2'] = df_PandQ_Event.groupby(['Code','Date'])['E_dummy2'].sum().tolist()\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy3'] = df_PandQ_Event.groupby(['Code','Date'])['E_dummy3'].sum().tolist()\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy4'] = df_PandQ_Event.groupby(['Code','Date'])['E_dummy4'].sum().tolist()\n",
    "df_PandQ_Event_dropMultiEvent['JumpHigh'] = np.where((df_PandQ_Event_dropMultiEvent['Open_t+1'].values  - df_PandQ_Event_dropMultiEvent['High'].values)>0,1,0 )\n",
    "df_PandQ_Event_dropMultiEvent['JumpLow'] = np.where((df_PandQ_Event_dropMultiEvent['Open_t+1'].values  - df_PandQ_Event_dropMultiEvent['Low'].values)<0,1,0 )\n",
    "\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy1xJumpHigh'] = df_PandQ_Event_dropMultiEvent['E_dummy1'].values*df_PandQ_Event_dropMultiEvent['JumpHigh'].values\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy1xJumpLow']  = df_PandQ_Event_dropMultiEvent['E_dummy1'].values*df_PandQ_Event_dropMultiEvent['JumpLow'].values\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy2xJumpHigh'] = df_PandQ_Event_dropMultiEvent['E_dummy2'].values*df_PandQ_Event_dropMultiEvent['JumpHigh'].values\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy2xJumpLow']  = df_PandQ_Event_dropMultiEvent['E_dummy2'].values*df_PandQ_Event_dropMultiEvent['JumpLow'].values\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy3xJumpHigh'] = df_PandQ_Event_dropMultiEvent['E_dummy3'].values*df_PandQ_Event_dropMultiEvent['JumpHigh'].values\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy3xJumpLow']  = df_PandQ_Event_dropMultiEvent['E_dummy3'].values*df_PandQ_Event_dropMultiEvent['JumpLow'].values\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy4xJumpHigh'] = df_PandQ_Event_dropMultiEvent['E_dummy4'].values*df_PandQ_Event_dropMultiEvent['JumpHigh'].values\n",
    "df_PandQ_Event_dropMultiEvent['E_dummy4xJumpLow']  = df_PandQ_Event_dropMultiEvent['E_dummy4'].values*df_PandQ_Event_dropMultiEvent['JumpLow'].values\n",
    "\n",
    "df_PandQ_Event_dropMultiEvent['E1xQ5'] = df_PandQ_Event_dropMultiEvent['E_dummy1'].values * df_PandQ_Event_dropMultiEvent['Fifth_Quintile'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PandQ_Event_dropMultiEventNa = df_PandQ_Event_dropMultiEvent.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "20140721    660\n",
       "20140722    659\n",
       "20140723    662\n",
       "20140724    661\n",
       "20140725    660\n",
       "20140728    663\n",
       "20140729    665\n",
       "20140730    665\n",
       "20140731    667\n",
       "20140801    667\n",
       "20140804    665\n",
       "20140805    669\n",
       "20140806    668\n",
       "20140807    671\n",
       "20140808    669\n",
       "20140811    667\n",
       "20140812    663\n",
       "20140813    660\n",
       "20140814    662\n",
       "20140815    662\n",
       "20140818    658\n",
       "20140819    659\n",
       "20140820    660\n",
       "20140821    662\n",
       "20140822    661\n",
       "20140825    659\n",
       "20140826    657\n",
       "20140827    658\n",
       "20140828    653\n",
       "20140829    654\n",
       "           ... \n",
       "20180604    847\n",
       "20180605    848\n",
       "20180606    847\n",
       "20180607    843\n",
       "20180608    841\n",
       "20180611    839\n",
       "20180612    833\n",
       "20180613    832\n",
       "20180614    832\n",
       "20180615    834\n",
       "20180619    832\n",
       "20180620    833\n",
       "20180621    833\n",
       "20180622    834\n",
       "20180625    837\n",
       "20180626    839\n",
       "20180627    841\n",
       "20180628    843\n",
       "20180629    844\n",
       "20180702    847\n",
       "20180703    851\n",
       "20180704    851\n",
       "20180705    851\n",
       "20180706    854\n",
       "20180709    857\n",
       "20180710    857\n",
       "20180711    857\n",
       "20180712    857\n",
       "20180713    857\n",
       "20180716    857\n",
       "Name: Code, Length: 974, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PandQ_Event_dropMultiEventNa.groupby('Date')['Code'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评分系统1 paired t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "df_blocktrade=df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['E_dummy1']==1]\n",
    "df_mortgage =df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['E_dummy2']==1]\n",
    "df_JumpHigh = df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['JumpHigh']==1]\n",
    "df_JumpLow = df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['JumpLow']==1]\n",
    "\n",
    "df_E1xJH = df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['E_dummy1xJumpHigh']==1]\n",
    "df_E1xJL = df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['E_dummy1xJumpLow']==1]\n",
    "df_E2xJH = df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['E_dummy2xJumpHigh']==1] \n",
    "df_E2xJL = df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['E_dummy2xJumpLow']==1]\n",
    "\n",
    "df_H1sd =  df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['HighThan1Std' ]==1]\n",
    "df_L1sd =  df_PandQ_Event_dropMultiEventNa[df_PandQ_Event_dropMultiEventNa['LowThan1Std' ]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E1_pvalues = sp.stats.ttest_rel(df_blocktrade['MeanF3'],df_blocktrade['MeanP5'])[1]\n",
    "E2_pvalues = sp.stats.ttest_rel(df_mortgage['MeanF3'],df_mortgage['MeanP5'])[1]\n",
    "JH_pvalues = sp.stats.ttest_rel(df_JumpHigh['MeanF3'],df_JumpHigh ['MeanP5'])[1]\n",
    "JL_pvalues = sp.stats.ttest_rel(df_JumpLow['MeanF3'],df_JumpLow['MeanP5'])[1]\n",
    "\n",
    "E1xJH_pvalues = sp.stats.ttest_rel(df_E1xJH['MeanF3'],df_E1xJH['MeanP5'])[1]\n",
    "E1xJL_pvalues = sp.stats.ttest_rel(df_E1xJL['MeanF3'],df_E1xJL['MeanP5'])[1]\n",
    "E2xJH_pvalues = sp.stats.ttest_rel(df_E2xJH['MeanF3'],df_E2xJH['MeanP5'])[1]\n",
    "E2xJL_pvalues = sp.stats.ttest_rel(df_E2xJL['MeanF3'],df_E2xJL['MeanP5'])[1]\n",
    "\n",
    "H1sd_pvalues = sp.stats.ttest_rel(df_H1sd['MeanF3'],df_H1sd['MeanP5'])[1]\n",
    "L1sd_pvalues = sp.stats.ttest_rel(df_L1sd['MeanF3'],df_L1sd['MeanP5'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评分方法二 正负比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依据paired T 系数决定影响方向\n",
    "E1_NPType = sp.stats.ttest_rel(df_blocktrade['MeanF3'],df_blocktrade['MeanP5'])[0] \n",
    "E2_NPType = sp.stats.ttest_rel(df_mortgage['MeanF3'],df_mortgage['MeanP5'])[0]\n",
    "JH_NPType = sp.stats.ttest_rel(df_JumpHigh['MeanF3'],df_JumpHigh ['MeanP5'])[0]\n",
    "JL_NPType = sp.stats.ttest_rel(df_JumpLow['MeanF3'],df_JumpLow['MeanP5'])[0]\n",
    "\n",
    "E1xJH_NPType = sp.stats.ttest_rel(df_E1xJH['MeanF3'],df_E1xJH['MeanP5'])[0]\n",
    "E1xJL_NPType = sp.stats.ttest_rel(df_E1xJL['MeanF3'],df_E1xJL['MeanP5'])[0]\n",
    "E2xJH_NPType = sp.stats.ttest_rel(df_E2xJH['MeanF3'],df_E2xJH['MeanP5'])[0]\n",
    "E2xJL_NPType = sp.stats.ttest_rel(df_E2xJL['MeanF3'],df_E2xJL['MeanP5'])[0]\n",
    "\n",
    "H1sd_NPType = sp.stats.ttest_rel(df_H1sd['MeanF3'],df_H1sd['MeanP5'])[0]\n",
    "L1sd_NPType = sp.stats.ttest_rel(df_L1sd['MeanF3'],df_L1sd['MeanP5'])[0]\n",
    "NPTypeList = [E1_NPType,E2_NPType,JH_NPType,JL_NPType ,E1xJH_NPType,E1xJL_NPType,E2xJH_NPType,E2xJL_NPType,H1sd_NPType,L1sd_NPType]\n",
    "NPTypeList_one = np.where(np.array(NPTypeList)>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PNrate(data,NPType):\n",
    "    data = data[(data<np.percentile(data,95)) & (data>np.percentile(data,5))]#去除極值\n",
    "    data_posi = len(data[data>0])\n",
    "    data_nega = len(data[data<=0])\n",
    "    if NPType == 1:\n",
    "        result =  data_posi/len(data)\n",
    "    else:\n",
    "        result =  data_nega/len(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E1_PNrate = PNrate(df_blocktrade['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[0])\n",
    "E2_PNrate = PNrate(df_mortgage['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[1])\n",
    "JH_PNrate = PNrate(df_JumpHigh['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[2])\n",
    "JL_PNrate = PNrate(df_JumpLow['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[3])\n",
    "\n",
    "E1xJH_PNrate = PNrate(df_E1xJH['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[4])\n",
    "E1xJL_PNrate = PNrate(df_E1xJL['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[5])\n",
    "E2xJH_PNrate = PNrate(df_E2xJH['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[6])\n",
    "E2xJL_PNrate = PNrate(df_E2xJL['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[7])\n",
    "\n",
    "H1sd_PNrate = PNrate(df_H1sd['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[8])\n",
    "L1sd_PNrate = PNrate(df_L1sd['MeanF3lessMeanP5%'].values,NPType=NPTypeList_one[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NPTypeList_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评分系统三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countsampleN(data) :\n",
    "    df_jumpHigh = pd.DataFrame()\n",
    "    df_jumpLow=pd.DataFrame()\n",
    "\n",
    "    df_random=pd.DataFrame()\n",
    "    ##４等分　隨機　大宗、質押　高開　低開\n",
    "    data_E1_count = len(data[data.E_dummy1 == 1])\n",
    "    data_E2_count = len(data[data.E_dummy2 == 1])\n",
    "    data_JH_count = len(data[data.JumpHigh==1])\n",
    "    data_JL_count = len(data[data.JumpLow==1])\n",
    "    data_nan_count = len(data[(data.E_dummy1==0)&(data.E_dummy2==0)&(data.JumpHigh==0)&(data.JumpLow==0)])\n",
    "    \n",
    "    random_count = np.where(data_E1_count+data_E2_count < 10 ,10,data_E1_count+data_E2_count)\n",
    "    random_count_jumpH = np.where(random_count>=data_JH_count,data_JH_count,random_count )\n",
    "    random_count_jumpL = np.where(random_count>=data_JL_count,data_JL_count,random_count )\n",
    "    random_count_nan  = np.where(random_count>=data_nan_count,data_nan_count,random_count )\n",
    "    \n",
    "    #random_count_jumpH = np.where( random_count_jumpH ==0 ,10,random_count_jumpH  )\n",
    "    #random_count_jumpL = np.where( random_count_jumpL ==0 ,10,random_count_jumpL )   \n",
    "    \n",
    "    df_E = data[(data.E_dummy1==1)|(data.E_dummy2==1)]\n",
    "    if random_count_jumpH ==0:\n",
    "        df_jumpHigh = data[data.JumpHigh == 1]\n",
    "    else:\n",
    "        df_jumpHigh = data[data.JumpHigh == 1].sample(random_count_jumpH)\n",
    "   \n",
    "    if random_count_jumpL ==0:\n",
    "        df_jumpLow = data[data.JumpLow == 1]\n",
    "    else:\n",
    "        df_jumpLow = data[data.JumpLow == 1].sample(random_count_jumpL)\n",
    "\n",
    "    df_nan =data[(data.E_dummy1==0)&(data.E_dummy2==0)&(data.JumpHigh==0)&(data.JumpLow==0)]\n",
    "    if random_count_nan   ==0:\n",
    "        df_random = df_nan\n",
    "    else:\n",
    "        df_random = df_nan.sample(random_count_nan)\n",
    "\n",
    "\n",
    "    data = pd.concat([df_random,df_E,df_jumpHigh,df_jumpLow] ).drop_duplicates().reset_index(drop=True)\n",
    "    return [len(df_E),len(df_jumpHigh),len(df_jumpLow),len(df_random),len(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20140721</th>\n",
       "      <td>[0, 10, 10, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140722</th>\n",
       "      <td>[0, 10, 4, 10, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140723</th>\n",
       "      <td>[0, 9, 10, 10, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140724</th>\n",
       "      <td>[0, 10, 10, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140725</th>\n",
       "      <td>[0, 10, 5, 10, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140728</th>\n",
       "      <td>[14, 14, 3, 14, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140729</th>\n",
       "      <td>[23, 24, 16, 24, 84]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140730</th>\n",
       "      <td>[10, 11, 11, 11, 41]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140731</th>\n",
       "      <td>[15, 15, 15, 15, 59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140801</th>\n",
       "      <td>[11, 11, 11, 11, 44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140804</th>\n",
       "      <td>[13, 13, 3, 13, 42]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140805</th>\n",
       "      <td>[11, 11, 11, 11, 43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140806</th>\n",
       "      <td>[12, 14, 10, 14, 49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140807</th>\n",
       "      <td>[12, 12, 12, 12, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140808</th>\n",
       "      <td>[15, 15, 6, 15, 51]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140811</th>\n",
       "      <td>[10, 10, 9, 10, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140812</th>\n",
       "      <td>[21, 21, 16, 21, 78]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140813</th>\n",
       "      <td>[19, 20, 14, 20, 71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140814</th>\n",
       "      <td>[20, 16, 20, 20, 75]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140815</th>\n",
       "      <td>[18, 18, 6, 18, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140818</th>\n",
       "      <td>[10, 10, 2, 10, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140819</th>\n",
       "      <td>[22, 22, 21, 22, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140820</th>\n",
       "      <td>[14, 15, 15, 15, 59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140821</th>\n",
       "      <td>[15, 15, 8, 15, 53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140822</th>\n",
       "      <td>[13, 13, 13, 13, 52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140825</th>\n",
       "      <td>[13, 13, 13, 13, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140826</th>\n",
       "      <td>[14, 14, 14, 14, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140827</th>\n",
       "      <td>[16, 16, 16, 16, 64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140828</th>\n",
       "      <td>[14, 11, 14, 14, 52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140829</th>\n",
       "      <td>[20, 20, 3, 20, 62]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180604</th>\n",
       "      <td>[13, 13, 13, 13, 52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180605</th>\n",
       "      <td>[30, 30, 17, 30, 106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180606</th>\n",
       "      <td>[38, 39, 28, 39, 138]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180607</th>\n",
       "      <td>[30, 17, 30, 30, 105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180608</th>\n",
       "      <td>[27, 26, 27, 27, 104]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180611</th>\n",
       "      <td>[11, 12, 12, 12, 47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180612</th>\n",
       "      <td>[31, 32, 22, 32, 115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180613</th>\n",
       "      <td>[32, 9, 32, 32, 105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180614</th>\n",
       "      <td>[32, 32, 32, 32, 126]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180615</th>\n",
       "      <td>[20, 5, 20, 20, 63]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180619</th>\n",
       "      <td>[16, 3, 16, 16, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180620</th>\n",
       "      <td>[45, 47, 24, 47, 160]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180621</th>\n",
       "      <td>[56, 8, 56, 56, 171]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180622</th>\n",
       "      <td>[66, 67, 10, 67, 203]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180625</th>\n",
       "      <td>[19, 10, 19, 19, 65]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180626</th>\n",
       "      <td>[52, 52, 14, 52, 167]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180627</th>\n",
       "      <td>[40, 24, 41, 41, 142]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180628</th>\n",
       "      <td>[38, 23, 39, 39, 135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180629</th>\n",
       "      <td>[32, 34, 11, 34, 108]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180702</th>\n",
       "      <td>[10, 10, 10, 10, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180703</th>\n",
       "      <td>[43, 44, 32, 44, 158]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180704</th>\n",
       "      <td>[29, 18, 30, 30, 106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180705</th>\n",
       "      <td>[26, 16, 26, 26, 93]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180706</th>\n",
       "      <td>[20, 21, 21, 21, 82]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180709</th>\n",
       "      <td>[13, 13, 13, 13, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180710</th>\n",
       "      <td>[0, 10, 10, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180711</th>\n",
       "      <td>[0, 10, 10, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180712</th>\n",
       "      <td>[0, 10, 10, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180713</th>\n",
       "      <td>[0, 10, 10, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180716</th>\n",
       "      <td>[0, 10, 10, 10, 30]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0\n",
       "Date                           \n",
       "20140721    [0, 10, 10, 10, 30]\n",
       "20140722     [0, 10, 4, 10, 24]\n",
       "20140723     [0, 9, 10, 10, 29]\n",
       "20140724    [0, 10, 10, 10, 30]\n",
       "20140725     [0, 10, 5, 10, 25]\n",
       "20140728    [14, 14, 3, 14, 45]\n",
       "20140729   [23, 24, 16, 24, 84]\n",
       "20140730   [10, 11, 11, 11, 41]\n",
       "20140731   [15, 15, 15, 15, 59]\n",
       "20140801   [11, 11, 11, 11, 44]\n",
       "20140804    [13, 13, 3, 13, 42]\n",
       "20140805   [11, 11, 11, 11, 43]\n",
       "20140806   [12, 14, 10, 14, 49]\n",
       "20140807   [12, 12, 12, 12, 48]\n",
       "20140808    [15, 15, 6, 15, 51]\n",
       "20140811    [10, 10, 9, 10, 39]\n",
       "20140812   [21, 21, 16, 21, 78]\n",
       "20140813   [19, 20, 14, 20, 71]\n",
       "20140814   [20, 16, 20, 20, 75]\n",
       "20140815    [18, 18, 6, 18, 60]\n",
       "20140818    [10, 10, 2, 10, 32]\n",
       "20140819   [22, 22, 21, 22, 85]\n",
       "20140820   [14, 15, 15, 15, 59]\n",
       "20140821    [15, 15, 8, 15, 53]\n",
       "20140822   [13, 13, 13, 13, 52]\n",
       "20140825   [13, 13, 13, 13, 50]\n",
       "20140826   [14, 14, 14, 14, 55]\n",
       "20140827   [16, 16, 16, 16, 64]\n",
       "20140828   [14, 11, 14, 14, 52]\n",
       "20140829    [20, 20, 3, 20, 62]\n",
       "...                         ...\n",
       "20180604   [13, 13, 13, 13, 52]\n",
       "20180605  [30, 30, 17, 30, 106]\n",
       "20180606  [38, 39, 28, 39, 138]\n",
       "20180607  [30, 17, 30, 30, 105]\n",
       "20180608  [27, 26, 27, 27, 104]\n",
       "20180611   [11, 12, 12, 12, 47]\n",
       "20180612  [31, 32, 22, 32, 115]\n",
       "20180613   [32, 9, 32, 32, 105]\n",
       "20180614  [32, 32, 32, 32, 126]\n",
       "20180615    [20, 5, 20, 20, 63]\n",
       "20180619    [16, 3, 16, 16, 50]\n",
       "20180620  [45, 47, 24, 47, 160]\n",
       "20180621   [56, 8, 56, 56, 171]\n",
       "20180622  [66, 67, 10, 67, 203]\n",
       "20180625   [19, 10, 19, 19, 65]\n",
       "20180626  [52, 52, 14, 52, 167]\n",
       "20180627  [40, 24, 41, 41, 142]\n",
       "20180628  [38, 23, 39, 39, 135]\n",
       "20180629  [32, 34, 11, 34, 108]\n",
       "20180702   [10, 10, 10, 10, 40]\n",
       "20180703  [43, 44, 32, 44, 158]\n",
       "20180704  [29, 18, 30, 30, 106]\n",
       "20180705   [26, 16, 26, 26, 93]\n",
       "20180706   [20, 21, 21, 21, 82]\n",
       "20180709   [13, 13, 13, 13, 50]\n",
       "20180710    [0, 10, 10, 10, 30]\n",
       "20180711    [0, 10, 10, 10, 30]\n",
       "20180712    [0, 10, 10, 10, 30]\n",
       "20180713    [0, 10, 10, 10, 30]\n",
       "20180716    [0, 10, 10, 10, 30]\n",
       "\n",
       "[974 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_PandQ_Event_dropMultiEventNa.groupby('Date').apply(lambda x : countsampleN(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_efficiency_rank(data):\n",
    "    df_jumpHigh = pd.DataFrame()\n",
    "    df_jumpLow=pd.DataFrame()\n",
    "\n",
    "    df_random=pd.DataFrame()\n",
    "    ##４等分　隨機　大宗、質押　高開　低開\n",
    "    data_E1_count = len(data[data.E_dummy1 == 1])\n",
    "    data_E2_count = len(data[data.E_dummy2 == 1])\n",
    "    data_JH_count = len(data[data.JumpHigh==1])\n",
    "    data_JL_count = len(data[data.JumpLow==1])\n",
    "    data_nan_count = len(data[(data.E_dummy1==0)&(data.E_dummy2==0)&(data.JumpHigh==0)&(data.JumpLow==0)])\n",
    "    \n",
    "    random_count = np.where(data_E1_count+data_E2_count < 10 ,10,data_E1_count+data_E2_count)\n",
    "    random_count_jumpH = np.where(random_count>=data_JH_count,data_JH_count,random_count )\n",
    "    random_count_jumpL = np.where(random_count>=data_JL_count,data_JL_count,random_count )\n",
    "    random_count_nan  = np.where(random_count>=data_nan_count,data_nan_count,random_count )\n",
    "    \n",
    "    #random_count_jumpH = np.where( random_count_jumpH ==0 ,10,random_count_jumpH  )\n",
    "    #random_count_jumpL = np.where( random_count_jumpL ==0 ,10,random_count_jumpL )   \n",
    "    \n",
    "    df_E = data[(data.E_dummy1==1)|(data.E_dummy2==1)]\n",
    "    if random_count_jumpH ==0:\n",
    "        df_jumpHigh = data[data.JumpHigh == 1]\n",
    "    else:\n",
    "        df_jumpHigh = data[data.JumpHigh == 1].sample(random_count_jumpH)\n",
    "   \n",
    "    if random_count_jumpL ==0:\n",
    "        df_jumpLow = data[data.JumpLow == 1]\n",
    "    else:\n",
    "        df_jumpLow = data[data.JumpLow == 1].sample(random_count_jumpL)\n",
    "\n",
    "    df_nan =data[(data.E_dummy1==0)&(data.E_dummy2==0)&(data.JumpHigh==0)&(data.JumpLow==0)]\n",
    "    if random_count_nan   ==0:\n",
    "        df_random = df_nan\n",
    "    else:\n",
    "        df_random = df_nan.sample(random_count_nan)\n",
    "\n",
    "\n",
    "    data = pd.concat([df_random,df_E,df_jumpHigh,df_jumpLow] ).drop_duplicates().reset_index(drop=True)\n",
    "    y=data['MeanF3lessMeanP5%']\n",
    "    X=sm.add_constant(data[['E_dummy1',\n",
    "      'E_dummy2', 'JumpHigh', 'JumpLow',\n",
    "       'E_dummy1xJumpHigh', 'E_dummy1xJumpLow', 'E_dummy2xJumpHigh',\n",
    "       'E_dummy2xJumpLow','LowThan1Std','HighThan1Std','First_Quintile','Fifth_Quintile']])\n",
    "    Weight = data['Cap']\n",
    "    result = sm.WLS(y,X,weights=Weight).fit()\n",
    "    return  [ result.pvalues.tolist() , result.tvalues.tolist(), result.params.tolist(),result.f_pvalue ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:1036: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "df_PandQ_Event_dropMultiEventNa['MeanF3'] = np.mean(df_PandQ_Event_dropMultiEventNa[['Volume_t0', 'Volume_t1', 'Volume_t2',\n",
    "       'Volume_t3']],axis=1)\n",
    "df_PandQ_Event_dropMultiEventNa['MeanP5'] = np.mean(df_PandQ_Event_dropMultiEventNa[['Volume_t-5', 'Volume_t-4', 'Volume_t-3',\n",
    "       'Volume_t-2', 'Volume_t-1']],axis=1)\n",
    "df_PandQ_Event_dropMultiEventNa['MeanF3lessMeanP5%'] = (df_PandQ_Event_dropMultiEventNa['MeanF3'].values- df_PandQ_Event_dropMultiEventNa['MeanP5'].values)/df_PandQ_Event_dropMultiEventNa['MeanP5'].values\n",
    "\n",
    "pvalue_list= df_PandQ_Event_dropMultiEventNa.groupby('Date').apply(lambda x : random_efficiency_rank(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:1036: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in less\n",
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "from progressbar import ProgressBar\n",
    "\n",
    "total = 50\n",
    "\n",
    "significant_E1 =[]\n",
    "significant_E2 = []\n",
    "significant_jumpH = []\n",
    "significant_jumpL = []\n",
    "significant_e1jumpH = []\n",
    "significant_e1jumpL = []\n",
    "significant_e2jumpH =[]\n",
    "significant_e2jumpL=[]\n",
    "significant_HighThan1Std = []\n",
    "significant_LowThan1Std=[]\n",
    "\n",
    "E_dummy1_Beta_pnratios=[]\n",
    "E_dummy2_Beta_pnratios=[]\n",
    "JumpHigh_Beta_pnratios=[]\n",
    "JumpLow_Beta_pnratios=[]\n",
    "E_dummy1xJumpHigh_Beta_pnratios=[]\n",
    "E_dummy1xJumpLow_Beta_pnratios=[]\n",
    "E_dummy2xJumpHigh_Beta_pnratios=[]\n",
    "E_dummy2xJumpLow_Beta_pnratios=[]\n",
    "HighThan1Std_Beta_pnratios=[]\n",
    "LowThan1Std_Beta_pnratios=[]\n",
    "\n",
    "FAMAT_E_dummy1 =[]\n",
    "FAMAT_E_dummy2 =[]\n",
    "FAMAT_JumpHigh =[]\n",
    "FAMAT_JumpLow = []\n",
    "FAMAT_E_dummy1xJumpHigh=[]\n",
    "FAMAT_E_dummy1xJumpLow=[]\n",
    "FAMAT_E_dummy2xJumpHigh=[]\n",
    "FAMAT_E_dummy2xJumpLow=[]\n",
    "FAMAT_HighThan1Std=[]\n",
    "FAMAT_LowThan1Std=[]\n",
    "\n",
    "p_value_all=[]\n",
    "pbar = ProgressBar().start()\n",
    "for i in range(0,50):\n",
    "    pvalue_list= df_PandQ_Event_dropMultiEventNa.groupby('Date').apply(lambda x : random_efficiency_rank(x))\n",
    "#pvalue\n",
    "    E_dummy1_P  =[ x[0][1] for x in pvalue_list ]\n",
    "    E_dummy2_P  =[ x[0][2] for x in pvalue_list ]\n",
    "\n",
    "\n",
    "    JumpHigh_P = [ x[0][3] for x in pvalue_list ]\n",
    "    JumpLow_P  = [ x[0][4] for x in pvalue_list ]\n",
    "\n",
    "    E_dummy1xJumpHigh_P = [ x[0][5] for x in pvalue_list ]\n",
    "    E_dummy1xJumpLow_P = [ x[0][6] for x in pvalue_list ]\n",
    "    E_dummy2xJumpHigh_P = [ x[0][7] for x in pvalue_list ]\n",
    "    E_dummy2xJumpLow_P = [ x[0][8] for x in pvalue_list ]\n",
    "\n",
    "    LowThan1Std = [ x[0][9] for x in pvalue_list ]\n",
    "    HighThan1Std = [ x[0][10] for x in pvalue_list ]\n",
    "#显著比例\n",
    "\n",
    "    significant_E1.append(sum(np.where(np.array(E_dummy1_P)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1'].sum().mean()].count())\n",
    " #大宗\n",
    "    significant_E2.append(sum(np.where(np.array(E_dummy2_P)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2'].sum().mean()].count())\n",
    " #股權\n",
    "\n",
    "    significant_jumpH.append(  sum(np.where(np.array(JumpHigh_P)<0.05,1,0 ))/len(E_dummy1_P))\n",
    "    significant_jumpL.append( sum(np.where(np.array(JumpLow_P)<0.05,1,0 ))/len(E_dummy1_P))\n",
    "\n",
    "    significant_e1jumpH.append( sum(np.where(np.array(E_dummy1xJumpHigh_P)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1xJumpHigh'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1xJumpHigh'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1xJumpHigh'].sum().mean()].count())\n",
    "    significant_e2jumpH.append( sum(np.where(np.array(E_dummy2xJumpHigh_P)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2xJumpHigh'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2xJumpHigh'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2xJumpHigh'].sum().mean()].count())\n",
    "\n",
    "    significant_e1jumpL.append( sum(np.where(np.array(E_dummy1xJumpLow_P)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1xJumpLow'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1xJumpLow'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy1xJumpLow'].sum().mean()].count())\n",
    "    significant_e2jumpL.append( sum(np.where(np.array(E_dummy2xJumpLow_P)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2xJumpLow'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2xJumpLow'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['E_dummy2xJumpLow'].sum().mean()].count())\n",
    "\n",
    "    significant_LowThan1Std.append( sum(np.where(np.array(LowThan1Std)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum().mean()].count())\n",
    "    significant_HighThan1Std.append( sum(np.where(np.array(HighThan1Std)<0.05,1,0 ))/df_PandQ_Event_dropMultiEventNa.groupby('Date')['HighThan1Std'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['HighThan1Std'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['HighThan1Std'].sum().mean()].count())\n",
    "#beta\n",
    "\n",
    "    E_dummy1_Beta  =[ x[2][1] for x in pvalue_list ]\n",
    "    E_dummy2_Beta  =[ x[2][2] for x in pvalue_list ]\n",
    "\n",
    "    JumpHigh_Beta = [ x[2][3] for x in pvalue_list ]\n",
    "    JumpLow_Beta  = [ x[2][4] for x in pvalue_list ]\n",
    "\n",
    "    E_dummy1xJumpHigh_Beta = [ x[2][5] for x in pvalue_list ]\n",
    "    E_dummy1xJumpLow_Beta = [ x[2][6] for x in pvalue_list ]\n",
    "    E_dummy2xJumpHigh_Beta = [ x[2][7] for x in pvalue_list ]\n",
    "    E_dummy2xJumpLow_Beta = [ x[2][8] for x in pvalue_list ]\n",
    "\n",
    "    LowThan1Std_Beta = [ x[2][9] for x in pvalue_list ]\n",
    "    HighThan1Std_Beta = [ x[2][10] for x in pvalue_list ]\n",
    "#正负比例\n",
    "    E_dummy1_Beta_pnratios.append( sum(np.where(np.array(E_dummy1_Beta)>0,1,0 ))/ sum(np.where(np.array(E_dummy1_Beta)<0,1,0 )) ) #大宗\n",
    "    #-E_dummy2_Beta\n",
    "    E_dummy2_Beta_pnratios.append( sum(np.where(np.array(E_dummy2_Beta)<0,1,0 ))/ sum(np.where(np.array(E_dummy2_Beta)>0,1,0 )) ) #股權\n",
    "\n",
    "    JumpHigh_Beta_pnratios.append( sum(np.where(np.array(JumpHigh_Beta)>0.00001,1,0 ))/ sum(np.where(np.array(JumpHigh_Beta)<0,1,0 )))\n",
    "    #-JumpLow\n",
    "    JumpLow_Beta_pnratios.append( sum(np.where(np.array(JumpLow_Beta)<0,1,0 ))/ sum(np.where(np.array(JumpLow_Beta)>0.00001,1,0 )))\n",
    "\n",
    "    E_dummy1xJumpHigh_Beta_pnratios.append( sum(np.where(np.array(E_dummy1xJumpHigh_Beta)>0,1,0 ))/sum(np.where(np.array(E_dummy1xJumpHigh_Beta)<0,1,0 )) )\n",
    "    E_dummy1xJumpLow_Beta_pnratios.append( sum(np.where(np.array(E_dummy2xJumpHigh_Beta)>0,1,0 ))/ sum(np.where(np.array(E_dummy2xJumpHigh_Beta)<0,1,0 )) )\n",
    "\n",
    "    E_dummy2xJumpHigh_Beta_pnratios.append( sum(np.where(np.array(E_dummy1xJumpLow_Beta)>0,1,0 ))/ sum(np.where(np.array(E_dummy1xJumpLow_Beta)<0,1,0 )) )\n",
    "    #-E_dummy2xJumpLow\n",
    "    E_dummy2xJumpLow_Beta_pnratios.append( sum(np.where(np.array(E_dummy2xJumpLow_Beta)<0,1,0 ))/ sum(np.where(np.array(E_dummy2xJumpLow_Beta)>0,1,0 )) )\n",
    "\n",
    "    LowThan1Std_Beta_pnratios.append( sum(np.where(np.array(LowThan1Std_Beta)>0,1,0 ))/sum(np.where(np.array(LowThan1Std_Beta)<0,1,0 )) )\n",
    "    #-HighThan1Std\n",
    "    HighThan1Std_Beta_pnratios.append( sum(np.where(np.array(HighThan1Std_Beta)<0,1,0 ))/sum(np.where(np.array(HighThan1Std_Beta)>0,1,0 ))) \n",
    "#評分標準三 FAMAT\n",
    "    FAMAT_E_dummy1.append( np.mean(E_dummy1_Beta)/(np.std(E_dummy1_Beta)/np.sqrt(len(E_dummy1_Beta))))\n",
    "    FAMAT_E_dummy2.append( np.mean(E_dummy2_Beta)/(np.std(E_dummy2_Beta)/np.sqrt(len(E_dummy2_Beta))))\n",
    "\n",
    "    FAMAT_JumpHigh.append( np.mean(JumpHigh_Beta)/(np.std(JumpHigh_Beta)/np.sqrt(len(JumpHigh_Beta))))\n",
    "    FAMAT_JumpLow.append( np.mean(JumpLow_Beta)/(np.std(JumpLow_Beta)/np.sqrt(len(JumpLow_Beta))))\n",
    "\n",
    "    FAMAT_E_dummy1xJumpHigh.append( np.mean(E_dummy1xJumpHigh_Beta)/(np.std(E_dummy1xJumpHigh_Beta)/np.sqrt(len(E_dummy1xJumpHigh_Beta))))\n",
    "    FAMAT_E_dummy1xJumpLow.append( np.mean(E_dummy1xJumpLow_Beta)/(np.std(E_dummy1xJumpLow_Beta)/np.sqrt(len(E_dummy1xJumpLow_Beta))))\n",
    "    FAMAT_E_dummy2xJumpHigh.append( np.mean(E_dummy2xJumpHigh_Beta)/(np.std(E_dummy2xJumpHigh_Beta)/np.sqrt(len(E_dummy2xJumpHigh_Beta))))\n",
    "    FAMAT_E_dummy2xJumpLow.append( np.mean(E_dummy2xJumpLow_Beta)/(np.std(E_dummy2xJumpLow_Beta)/np.sqrt(len(E_dummy2xJumpLow_Beta))))\n",
    "\n",
    "    FAMAT_HighThan1Std.append( np.mean(HighThan1Std_Beta)/(np.std(HighThan1Std_Beta)/np.sqrt(len(HighThan1Std_Beta))))\n",
    "    FAMAT_LowThan1Std.append( np.mean(LowThan1Std_Beta)/(np.std(LowThan1Std_Beta)/np.sqrt(len(LowThan1Std_Beta))))\n",
    "\n",
    "\n",
    "    p_value_all.append(np.percentile(pd.DataFrame([ x[2] for x in pvalue_list ]),70))\n",
    "\n",
    "    pbar.update(int((i/(total-1))*100))\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Rank_System = pd.DataFrame()\n",
    "df_Rank_System['Event&Indicators'] = ['BlockTrade','Mortgage','JumpHigh','JumpLow','BlockTrade_x_JumpHigh','Mortgage_x_JumpHigh',\n",
    "                                      'BlockTrade_x_JumpLow','Mortgage_x_JumpLow','HighThan1Std','LowThan1Std']\n",
    "df_Rank_System['Pair_T'] =  [E1_pvalues,E2_pvalues,JH_pvalues,JL_pvalues ,E1xJH_pvalues,E1xJL_pvalues,E2xJH_pvalues,E2xJL_pvalues,H1sd_pvalues,L1sd_pvalues]\n",
    "df_Rank_System['PN_Rate']= [E1_PNrate,E2_PNrate,JH_PNrate,JL_PNrate ,E1xJH_PNrate,E1xJL_PNrate,E2xJH_PNrate,E2xJL_PNrate,H1sd_PNrate,L1sd_PNrate]\n",
    "#因为系统三随机抽取50次 必须取平均\n",
    "df_Rank_System['Significant_Ratios'] = [np.mean(significant_E1),np.mean(significant_E2),np.mean(significant_jumpH),np.mean(significant_jumpL),\n",
    "                                        np.mean(significant_e1jumpH),np.mean(significant_e2jumpH),np.mean(significant_e1jumpL),np.mean(significant_e2jumpL),\n",
    "                                        np.mean(np.array(significant_HighThan1Std)*df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum().mean()].count()/len(df_PandQ_Event_dropMultiEventNa.groupby('Date').count())),\n",
    "                                        np.mean(np.array(significant_LowThan1Std)*df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum()[df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum()>  df_PandQ_Event_dropMultiEventNa.groupby('Date')['LowThan1Std'].sum().mean()].count()/len(df_PandQ_Event_dropMultiEventNa.groupby('Date').count()))]\n",
    "df_Rank_System['P/N_Ratios']=[np.mean(E_dummy1_Beta_pnratios),np.mean(E_dummy2_Beta_pnratios),np.mean(JumpHigh_Beta_pnratios),np.mean(JumpLow_Beta_pnratios),\n",
    "                              np.mean(E_dummy1xJumpHigh_Beta_pnratios),np.mean(E_dummy2xJumpHigh_Beta_pnratios),\n",
    "                              np.mean(E_dummy1xJumpLow_Beta_pnratios),np.mean(E_dummy2xJumpLow_Beta_pnratios),np.mean(HighThan1Std_Beta_pnratios),np.mean(LowThan1Std_Beta_pnratios)]\n",
    "df_Rank_System['FAMA_Tratios'] = [np.mean(FAMAT_E_dummy1),np.mean(FAMAT_E_dummy2),np.mean(FAMAT_JumpHigh),np.mean(FAMAT_JumpLow),\n",
    "                                  np.mean(FAMAT_E_dummy1xJumpHigh),np.mean(FAMAT_E_dummy2xJumpHigh),np.mean(FAMAT_E_dummy1xJumpLow),np.mean(FAMAT_E_dummy2xJumpLow),\n",
    "                                  np.mean(FAMAT_LowThan1Std),np.mean(FAMAT_HighThan1Std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant_ranking(data,Type):\n",
    "    if Type == 'Pair_T':\n",
    "        listbound = [0.01,0.025,0.05]\n",
    "        result = np.where(np.array(data)<=listbound[0],3,0)\n",
    "        result = np.where((np.array(data)>listbound[0])&(np.array(data)<=listbound[1]),2,result)\n",
    "        result = np.where((np.array(data)>listbound[1])&(np.array(data)<=listbound[2]),1,result)\n",
    "        result = np.where(np.array(data)>listbound[2],0,result)\n",
    "    if Type == 'PN_Rate':\n",
    "        listbound = [0.5,0.6,0.65,0.7]\n",
    "        result = np.where(np.array(data)<listbound[0],0,0)\n",
    "        result = np.where((np.array(data)>=listbound[0])&(np.array(data)<listbound[1]),1,result)\n",
    "        result = np.where((np.array(data)>=listbound[1])&(np.array(data)<listbound[2]),2,result)\n",
    "        result = np.where((np.array(data)>=listbound[2])&(np.array(data)<listbound[3]),3,result)\n",
    "        result = np.where(np.array(data)>=listbound[3],4,result)\n",
    "    if Type == 'Significant_Ratios':\n",
    "        listbound = [0.5,0.6,0.65,0.7]\n",
    "        result = np.where(np.array(data)<listbound[0],0,0)\n",
    "        result = np.where((np.array(data)>=listbound[0])&(np.array(data)<listbound[1]),1,result)\n",
    "        result = np.where((np.array(data)>=listbound[1])&(np.array(data)<listbound[2]),2,result)\n",
    "        result = np.where((np.array(data)>=listbound[2])&(np.array(data)<listbound[3]),3,result)\n",
    "        result = np.where(np.array(data)>=listbound[3],4,result)\n",
    "    if Type == 'P/N_Ratios':\n",
    "        listbound = [1.5,1.875,2.33]\n",
    "        result = np.where(np.array(data)<listbound[0],0,0)\n",
    "        result = np.where((np.array(data)>=listbound[0])&(np.array(data)<listbound[1]),1,result)\n",
    "        result = np.where((np.array(data)>=listbound[1])&(np.array(data)<listbound[2]),2,result)\n",
    "        result = np.where(np.array(data)>=listbound[2],3,result)\n",
    "    if Type == 'FAMA_Tratios':\n",
    "        listbound = [1.28,1.645,2]\n",
    "        result = np.where(abs(np.array(data))<listbound[0],0,0)\n",
    "        result = np.where((abs(np.array(data))>=listbound[0])&(abs(np.array(data))<listbound[1]),1,result)\n",
    "        result = np.where((abs(np.array(data))>=listbound[1])&(abs(np.array(data))<listbound[2]),2,result)\n",
    "        result = np.where(abs(np.array(data))>=listbound[2],3,result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Rank_System_Rank = pd.DataFrame()\n",
    "df_Rank_System_Rank['Event&Indicators'] = ['BlockTrade','Mortgage','JumpHigh','JumpLow','BlockTrade_x_JumpHigh','Mortgage_x_JumpHigh',\n",
    "                                      'BlockTrade_x_JumpLow','Mortgage_x_JumpLow','HighThan1Std','LowThan1Std']\n",
    "\n",
    "df_Rank_System_Rank['Pair_T'] = significant_ranking(df_Rank_System['Pair_T'],Type='Pair_T')\n",
    "df_Rank_System_Rank['PN_Rate']= significant_ranking(df_Rank_System['PN_Rate'] ,Type='PN_Rate')\n",
    "df_Rank_System_Rank['Significant_Ratios'] = significant_ranking(df_Rank_System['Significant_Ratios'] ,Type='Significant_Ratios')\n",
    "df_Rank_System_Rank['P/N_Ratios'] = significant_ranking(df_Rank_System['P/N_Ratios'] ,Type='P/N_Ratios')\n",
    "df_Rank_System_Rank['FAMAT_Ratios'] = significant_ranking(df_Rank_System['FAMA_Tratios'] ,Type='FAMA_Tratios')\n",
    "df_Rank_System_Rank['Sum_Rating'] = np.sum(df_Rank_System_Rank[['Pair_T','PN_Rate','Significant_Ratios','P/N_Ratios','FAMAT_Ratios']],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Rank_System_Rank.to_csv('Ranking_System_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event&amp;Indicators</th>\n",
       "      <th>Pair_T</th>\n",
       "      <th>PN_Rate</th>\n",
       "      <th>Significant_Ratios</th>\n",
       "      <th>P/N_Ratios</th>\n",
       "      <th>FAMAT_Ratios</th>\n",
       "      <th>Sum_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlockTrade</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JumpHigh</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JumpLow</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BlockTrade_x_JumpHigh</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mortgage_x_JumpHigh</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BlockTrade_x_JumpLow</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mortgage_x_JumpLow</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HighThan1Std</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LowThan1Std</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Event&Indicators  Pair_T  PN_Rate  Significant_Ratios  P/N_Ratios  \\\n",
       "0             BlockTrade       3        1                   0           0   \n",
       "1               Mortgage       0        1                   0           0   \n",
       "2               JumpHigh       3        3                   0           3   \n",
       "3                JumpLow       3        2                   0           0   \n",
       "4  BlockTrade_x_JumpHigh       3        3                   0           0   \n",
       "5    Mortgage_x_JumpHigh       3        0                   0           0   \n",
       "6   BlockTrade_x_JumpLow       3        2                   0           0   \n",
       "7     Mortgage_x_JumpLow       0        2                   0           0   \n",
       "8           HighThan1Std       3        4                   0           3   \n",
       "9            LowThan1Std       3        4                   0           0   \n",
       "\n",
       "   FAMAT_Ratios  Sum_Rating  \n",
       "0             0           4  \n",
       "1             0           1  \n",
       "2             3          12  \n",
       "3             3           8  \n",
       "4             0           6  \n",
       "5             0           3  \n",
       "6             0           5  \n",
       "7             0           2  \n",
       "8             0          10  \n",
       "9             3          10  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Rank_System_Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
